# -*- coding: utf-8 -*-
"""UX Analyzer Core - –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"""

import json
import re
import time
from typing import Dict, List, Any
from collections import defaultdict
from ux_analyzer_classes import (
    OpenRouterAPIWrapper, BriefManager, InterviewSummary, 
    ResearchFindings, CacheManager
)

# ========================================================================
# –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê
# ========================================================================
class AdvancedUXAnalyzer:
    def __init__(self, api_key: str):
        self.api_wrapper = OpenRouterAPIWrapper(api_key)
        self.brief_manager = BriefManager()
        self.cache = CacheManager()
        self.interview_summaries = []

    def set_brief(self, brief_content: str):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±—Ä–∏—Ñ–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        self.brief_manager.load_brief(brief_content)

    def analyze_transcripts(self, transcripts: List[str]) -> Dict:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤"""
        print("üß† –ù–∞—á–∏–Ω–∞—é –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑...")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–Ω—Ç–µ—Ä–≤—å—é
        if len(transcripts) < 3:
            print(f"‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 3 –∏–Ω—Ç–µ—Ä–≤—å—é –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞!")
            print(f"   –£ –≤–∞—Å: {len(transcripts)} –∏–Ω—Ç–µ—Ä–≤—å—é")

        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤—å—é
        interview_summaries = []
        for i, transcript in enumerate(transcripts):
            summary = self._deep_analyze_interview(transcript, i+1)
            interview_summaries.append(summary)

        self.interview_summaries = interview_summaries

        # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞
        return self._continue_analysis(interview_summaries, len(transcripts))

    def _deep_analyze_interview(self, transcript: str, interview_id: int) -> InterviewSummary:
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤—å—é"""
        context = self.brief_manager.get_brief_context()
        
        prompt = f"""{context}

–ü–†–û–ê–ù–ê–õ–ò–ó–ò–†–£–ô –≠–¢–û –ò–ù–¢–ï–†–í–¨–Æ #{interview_id} –ò –°–û–ó–î–ê–ô –î–ï–¢–ê–õ–¨–ù–û–ï –°–ê–ú–ú–ê–†–ò.

–¢–†–ê–ù–°–ö–†–ò–ü–¢ –ò–ù–¢–ï–†–í–¨–Æ:
{transcript[:8000]}

–°–û–ó–î–ê–ô JSON –°–¢–†–£–ö–¢–£–†–£:
{{
    "respondent_profile": {{
        "age_range": "–≤–æ–∑—Ä–∞—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞",
        "profession": "–ø—Ä–æ—Ñ–µ—Å—Å–∏—è",
        "tech_literacy": "—É—Ä–æ–≤–µ–Ω—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –≥—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç–∏",
        "experience_level": "–æ–ø—ã—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞",
        "main_goals": ["—Ü–µ–ª—å 1", "—Ü–µ–ª—å 2"],
        "pain_level": "—É—Ä–æ–≤–µ–Ω—å —Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏–∏ (1-10)"
    }},
    "key_themes": [
        {{
            "theme": "–Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã",
            "description": "–æ–ø–∏—Å–∞–Ω–∏–µ",
            "quotes": ["—Ü–∏—Ç–∞—Ç–∞ 1", "—Ü–∏—Ç–∞—Ç–∞ 2"],
            "importance": "–≤–∞–∂–Ω–æ—Å—Ç—å (1-10)"
        }}
    ],
    "pain_points": [
        {{
            "pain": "–æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã",
            "severity": "—Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å (1-10)",
            "frequency": "—á–∞—Å—Ç–æ—Ç–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π",
            "quotes": ["—Ü–∏—Ç–∞—Ç–∞ 1", "—Ü–∏—Ç–∞—Ç–∞ 2"],
            "impact": "–≤–ª–∏—è–Ω–∏–µ –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"
        }}
    ],
    "needs": [
        {{
            "need": "–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å",
            "type": "—è–≤–Ω–∞—è/—Å–∫—Ä—ã—Ç–∞—è",
            "priority": "–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (1-10)",
            "quotes": ["—Ü–∏—Ç–∞—Ç–∞ 1", "—Ü–∏—Ç–∞—Ç–∞ 2"]
        }}
    ],
    "insights": [
        "–∏–Ω—Å–∞–π—Ç 1",
        "–∏–Ω—Å–∞–π—Ç 2",
        "–∏–Ω—Å–∞–π—Ç 3"
    ],
    "emotional_journey": [
        {{
            "moment": "–º–æ–º–µ–Ω—Ç –≤ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–∏",
            "emotion": "—ç–º–æ—Ü–∏—è",
            "trigger": "—Ç—Ä–∏–≥–≥–µ—Ä",
            "intensity": "–∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å (1-10)",
            "quote": "—Ü–∏—Ç–∞—Ç–∞"
        }}
    ],
    "contradictions": [
        "–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ 1",
        "–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ 2"
    ],
    "quotes": [
        {{
            "text": "–ø–æ–ª–Ω–∞—è —Ü–∏—Ç–∞—Ç–∞ (–º–∏–Ω–∏–º—É–º 50 —Å–ª–æ–≤)",
            "context": "–∫–æ–Ω—Ç–µ–∫—Å—Ç",
            "importance": "–≤–∞–∂–Ω–æ—Å—Ç—å (1-10)",
            "theme": "–∫ –∫–∞–∫–æ–π —Ç–µ–º–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è"
        }}
    ],
    "business_pains": [
        {{
            "pain": "–±–∏–∑–Ω–µ—Å-–ø—Ä–æ–±–ª–µ–º–∞",
            "impact": "–≤–ª–∏—è–Ω–∏–µ –Ω–∞ –±–∏–∑–Ω–µ—Å",
            "quotes": ["—Ü–∏—Ç–∞—Ç–∞ 1", "—Ü–∏—Ç–∞—Ç–∞ 2"]
        }}
    ],
    "user_problems": [
        {{
            "problem": "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞",
            "severity": "—Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å (1-10)",
            "quotes": ["—Ü–∏—Ç–∞—Ç–∞ 1", "—Ü–∏—Ç–∞—Ç–∞ 2"]
        }}
    ],
    "opportunities": [
        "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å 1",
        "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å 2"
    ],
    "sentiment_score": "–æ–±—â–∏–π —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç (-10 –¥–æ +10)",
    "brief_related_findings": {{
        "goals_mentioned": ["—Ü–µ–ª—å 1", "—Ü–µ–ª—å 2"],
        "questions_answered": ["–≤–æ–ø—Ä–æ—Å 1", "–≤–æ–ø—Ä–æ—Å 2"],
        "metrics_impact": ["–º–µ—Ç—Ä–∏–∫–∞ 1", "–º–µ—Ç—Ä–∏–∫–∞ 2"]
    }}
}}

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏–∑ –∏–Ω—Ç–µ—Ä–≤—å—é
- –ö–∞–∂–¥–∞—è —Ü–∏—Ç–∞—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¢–û–ß–ù–û–ô (–º–∏–Ω–∏–º—É–º 50 —Å–ª–æ–≤)
- –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π –¥–µ—Ç–∞–ª–∏ - —Ç–æ–ª—å–∫–æ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
- –°–≤—è–∑—ã–≤–∞–π —Å —Ü–µ–ª—è–º–∏ –∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –±—Ä–∏—Ñ–∞
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–∞"""

        try:
            response = self.api_wrapper.generate_content(prompt, max_tokens=4000)
            data = self.api_wrapper.extract_json(response)
            
            # –°–æ–∑–¥–∞–µ–º InterviewSummary
            return InterviewSummary(
                interview_id=interview_id,
                respondent_profile=data.get('respondent_profile', {}),
                key_themes=data.get('key_themes', []),
                pain_points=data.get('pain_points', []),
                needs=data.get('needs', []),
                insights=data.get('insights', []),
                emotional_journey=data.get('emotional_journey', []),
                contradictions=data.get('contradictions', []),
                quotes=data.get('quotes', []),
                business_pains=data.get('business_pains', []),
                user_problems=data.get('user_problems', []),
                opportunities=data.get('opportunities', []),
                sentiment_score=float(data.get('sentiment_score', 0)),
                brief_related_findings=data.get('brief_related_findings', {})
            )
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–Ω—Ç–µ—Ä–≤—å—é {interview_id}: {e}")
            return self._create_empty_summary(interview_id)

    def _create_empty_summary(self, interview_id: int) -> InterviewSummary:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Å–∞–º–º–∞—Ä–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        return InterviewSummary(
            interview_id=interview_id,
            respondent_profile={},
            key_themes=[],
            pain_points=[],
            needs=[],
            insights=[],
            emotional_journey=[],
            contradictions=[],
            quotes=[],
            business_pains=[],
            user_problems=[],
            opportunities=[],
            sentiment_score=0.0,
            brief_related_findings={}
        )

    def _continue_analysis(self, interview_summaries: List[InterviewSummary], total_interviews: int) -> Dict:
        """–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–Ω—Ç–µ—Ä–≤—å—é"""
        print("üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞—é –∞–Ω–∞–ª–∏–∑...")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—É—â–∏—Ö –º–µ—Ç—Ä–∏–∫
        current_metrics = self._generate_current_metrics(interview_summaries)

        # –ö—Ä–æ—Å—Å-–∞–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ—Ä–≤—å—é
        cross_analysis = self._cross_analyze_interviews(interview_summaries)

        # –í—ã—è–≤–ª–µ–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        patterns = self._identify_behavioral_patterns(interview_summaries, cross_analysis)

        # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∞—É–¥–∏—Ç–æ—Ä–∏–∏
        segments = self._segment_audience(interview_summaries, patterns)

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω
        personas = self._create_personas(segments, interview_summaries)

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –Ω–∞—Ö–æ–¥–æ–∫
        findings = self._generate_final_findings(
            interview_summaries, cross_analysis, patterns, segments, personas
        )

        findings.current_metrics = current_metrics

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        recommendations = self._generate_recommendations(findings.key_insights)

        # –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –±—Ä–∏—Ñ–∞
        brief_answers = self._analyze_brief_questions(interview_summaries, findings)
        findings.brief_answers = brief_answers

        # –û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π
        goal_achievement = self._assess_goal_achievement(findings, interview_summaries)
        findings.goal_achievement = goal_achievement

        return {
            'base_analysis': {
                'segments': segments,
                'problems': findings.key_insights,
                'insights': self._format_insights_for_report(findings.key_insights),
                'user_journey_issues': []
            },
            'recommendations': recommendations,
            'interview_summaries': interview_summaries,
            'findings': findings,
            'total_interviews': total_interviews,
            'current_metrics': current_metrics,
            'personas': personas,
            'brief_data': self.brief_manager.brief_data if self.brief_manager.has_brief else None,
            'brief_answers': brief_answers,
            'goal_achievement': goal_achievement
        }

    def _generate_current_metrics(self, summaries: List[InterviewSummary]) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—É—â–∏—Ö –º–µ—Ç—Ä–∏–∫"""
        if not summaries:
            return {}

        total_pains = sum(len(s.pain_points) for s in summaries)
        total_needs = sum(len(s.needs) for s in summaries)
        total_emotions = sum(len(s.emotional_journey) for s in summaries)
        
        positive_emotions = sum(1 for s in summaries for e in s.emotional_journey 
                              if isinstance(e, dict) and isinstance(e.get('intensity', 0), (int, float)) and e.get('intensity', 0) > 5)
        negative_emotions = sum(1 for s in summaries for e in s.emotional_journey 
                              if isinstance(e, dict) and isinstance(e.get('intensity', 0), (int, float)) and e.get('intensity', 0) < 5)

        # –û—Ü–µ–Ω–∫–∞ NPS
        sentiments = [s.sentiment_score for s in summaries if s.sentiment_score != 0]
        if sentiments:
            avg_sentiment = sum(sentiments) / len(sentiments)
            estimated_nps = int(avg_sentiment * 10)
        else:
            estimated_nps = '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö'

        # –†–∏—Å–∫ –æ—Ç—Ç–æ–∫–∞
        avg_pains = total_pains / len(summaries) if summaries else 0
        if isinstance(estimated_nps, int):
            if estimated_nps < -30 or avg_pains > 7:
                churn_risk = '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π'
            elif estimated_nps < 0 or avg_pains > 5:
                churn_risk = '–í—ã—Å–æ–∫–∏–π'
            elif estimated_nps < 30 or avg_pains > 3:
                churn_risk = '–°—Ä–µ–¥–Ω–∏–π'
            else:
                churn_risk = '–ù–∏–∑–∫–∏–π'
        else:
            churn_risk = '–°—Ä–µ–¥–Ω–∏–π' if avg_pains > 4 else '–ù–∏–∑–∫–∏–π'

        return {
            'estimated_nps': estimated_nps,
            'churn_risk': churn_risk,
            'avg_pains_per_user': round(avg_pains, 1),
            'avg_needs_per_user': round(total_needs / len(summaries), 1) if summaries else 0,
            'negative_emotion_ratio': round(negative_emotions / total_emotions * 100 if total_emotions > 0 else 0),
            'total_emotions_analyzed': total_emotions,
            'sample_size': len(summaries)
        }

    def _cross_analyze_interviews(self, summaries: List[InterviewSummary]) -> Dict:
        """–ö—Ä–æ—Å—Å-–∞–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ—Ä–≤—å—é"""
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã
            all_pains = []
            for summary in summaries:
                if hasattr(summary, 'pain_points') and summary.pain_points:
                    for pain in summary.pain_points:
                        if isinstance(pain, dict):
                            all_pains.append({
                                'pain': pain.get('pain', ''),
                                'severity': pain.get('severity', 0),
                                'interview_id': getattr(summary, 'interview_id', 0),
                                'quotes': pain.get('quotes', [])
                            })
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –ø—Ä–æ–±–ª–µ–º: {e}")
            all_pains = []

        try:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
            from collections import defaultdict
            pain_groups = defaultdict(list)
            for pain in all_pains:
                pain_text = pain.get('pain', '').lower() if pain.get('pain') else ''
                # –ü—Ä–æ—Å—Ç–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                for key in ['–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å', '–Ω–∞–≤–∏–≥–∞—Ü–∏—è', '—Å–∫–æ—Ä–æ—Å—Ç—å', '–æ—à–∏–±–∫–∞', '—Å–ª–æ–∂–Ω–æ', '–Ω–µ–ø–æ–Ω—è—Ç–Ω–æ']:
                    if key in pain_text:
                        pain_groups[key].append(pain)
                        break
                else:
                    pain_groups['–¥—Ä—É–≥–æ–µ'].append(pain)

            return {
                'pain_groups': dict(pain_groups),
                'total_pains': len(all_pains),
                'unique_pains': len(pain_groups)
            }
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–µ –ø—Ä–æ–±–ª–µ–º: {e}")
            return {
                'pain_groups': {},
                'total_pains': 0,
                'unique_pains': 0
            }

    def _identify_behavioral_patterns(self, summaries: List[InterviewSummary], cross_analysis: Dict) -> List[Dict]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        patterns = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω: –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–ª–µ–º—ã
        for group_name, pains in cross_analysis.get('pain_groups', {}).items():
            if len(pains) > 1:
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å
                severities = []
                for p in pains:
                    severity = p.get('severity', 0)
                    if isinstance(severity, str):
                        try:
                            severity = int(severity)
                        except (ValueError, TypeError):
                            severity = 0
                    severities.append(severity)
                
                patterns.append({
                    'pattern': f"–ü–æ–≤—Ç–æ—Ä—è—é—â–∞—è—Å—è –ø—Ä–æ–±–ª–µ–º–∞: {group_name}",
                    'frequency': len(pains),
                    'severity': max(severities) if severities else 0,
                    'description': f"–ü—Ä–æ–±–ª–µ–º–∞ '{group_name}' –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ {len(pains)} –∏–Ω—Ç–µ—Ä–≤—å—é"
                })

        return patterns

    def _segment_audience(self, summaries: List[InterviewSummary], patterns: List[Dict]) -> List[Dict]:
        """–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∞—É–¥–∏—Ç–æ—Ä–∏–∏"""
        segments = []
        
        # –ü—Ä–æ—Å—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ —É—Ä–æ–≤–Ω—é —Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏–∏
        high_frustration = [s for s in summaries if isinstance(s.sentiment_score, (int, float)) and s.sentiment_score < -3]
        medium_frustration = [s for s in summaries if isinstance(s.sentiment_score, (int, float)) and -3 <= s.sentiment_score <= 3]
        low_frustration = [s for s in summaries if isinstance(s.sentiment_score, (int, float)) and s.sentiment_score > 3]

        if high_frustration:
            segments.append({
                'name': '–í—ã—Å–æ–∫–∞—è —Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏—è',
                'size': len(high_frustration),
                'characteristics': '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å –≤—ã—Å–æ–∫–∏–º —É—Ä–æ–≤–Ω–µ–º –Ω–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–∞',
                'needs': '–°—Ä–æ—á–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞'
            })

        if medium_frustration:
            segments.append({
                'name': '–°—Ä–µ–¥–Ω—è—è —Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏—è',
                'size': len(medium_frustration),
                'characteristics': '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å —É–º–µ—Ä–µ–Ω–Ω—ã–º —É—Ä–æ–≤–Ω–µ–º —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏',
                'needs': '–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è'
            })

        if low_frustration:
            segments.append({
                'name': '–ù–∏–∑–∫–∞—è —Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏—è',
                'size': len(low_frustration),
                'characteristics': '–î–æ–≤–æ–ª—å–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏',
                'needs': '–ü–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞'
            })

        return segments

    def _create_personas(self, segments: List[Dict], summaries: List[InterviewSummary]) -> List[Dict]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω"""
        personas = []
        
        for i, segment in enumerate(segments, 1):
            # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤—å—é –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞
            segment_summaries = []
            if segment['name'] == '–í—ã—Å–æ–∫–∞—è —Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏—è':
                segment_summaries = [s for s in summaries if isinstance(s.sentiment_score, (int, float)) and s.sentiment_score < -3]
            elif segment['name'] == '–°—Ä–µ–¥–Ω—è—è —Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏—è':
                segment_summaries = [s for s in summaries if isinstance(s.sentiment_score, (int, float)) and -3 <= s.sentiment_score <= 3]
            else:
                segment_summaries = [s for s in summaries if isinstance(s.sentiment_score, (int, float)) and s.sentiment_score > 3]

            if segment_summaries:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∏–Ω—Ç–µ—Ä–≤—å—é –∫–∞–∫ –æ—Å–Ω–æ–≤—É –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã
                base_summary = segment_summaries[0]
                profile = base_summary.respondent_profile

                persona = {
                    'persona_id': f'P{i:03d}',
                    'name': f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {i}",
                    'based_on_interviews': [s.interview_id for s in segment_summaries[:3]],
                    'tagline': f"–ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–∞ '{segment['name']}'",
                    'description': f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–∑ —Å–µ–≥–º–µ–Ω—Ç–∞ '{segment['name']}' —Å {segment['characteristics']}",
                    'demographics': {
                        'age_range': profile.get('age_range', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
                        'profession': profile.get('profession', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
                        'tech_literacy': profile.get('tech_literacy', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
                    },
                    'goals': profile.get('main_goals', []),
                    'frustrations': [p.get('pain', '') for p in base_summary.pain_points[:3]],
                    'needs': [n.get('need', '') for n in base_summary.needs[:3]],
                    'real_quotes': [q.get('text', '') for q in base_summary.quotes[:3] if q.get('text')],
                    'typical_scenario': f"–¢–∏–ø–∏—á–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π –¥–ª—è {segment['name']}"
                }
                personas.append(persona)

        return personas

    def _generate_final_findings(self, summaries: List[InterviewSummary], cross_analysis: Dict, 
                                patterns: List[Dict], segments: List[Dict], personas: List[Dict]) -> ResearchFindings:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –Ω–∞—Ö–æ–¥–æ–∫"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–Ω—Å–∞–π—Ç—ã
        all_insights = []
        for summary in summaries:
            for insight in summary.insights:
                all_insights.append(insight)

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã
        all_pains = []
        for summary in summaries:
            for pain in summary.pain_points:
                if isinstance(pain, dict):
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞–µ–º severity
                    severity = pain.get('severity', 5)
                    if isinstance(severity, str):
                        try:
                            severity = int(severity)
                        except (ValueError, TypeError):
                            severity = 5
                    
                    all_pains.append({
                        'problem_title': pain.get('pain', ''),
                        'problem_description': pain.get('impact', ''),
                        'severity': severity,
                        'quotes': pain.get('quotes', []),
                        'affected_percentage': f"{len([s for s in summaries if any(p.get('pain', '') == pain.get('pain', '') for p in s.pain_points)]) / len(summaries) * 100:.0f}%"
                    })

        return ResearchFindings(
            executive_summary="–ê–Ω–∞–ª–∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä–≤—å—é –≤—ã—è–≤–∏–ª –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞.",
            key_insights=all_pains,
            behavioral_patterns=patterns,
            user_segments=segments,
            pain_points_map=cross_analysis.get('pain_groups', {}),
            opportunities=[o for s in summaries for o in s.opportunities],
            recommendations=[],
            risks=[],
            personas=personas
        )

    def _generate_recommendations(self, insights: List[Dict]) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        context = self.brief_manager.get_brief_context()
        
        prompt = f"""{context}

–ù–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å–æ–∑–¥–∞–π –ö–û–ù–ö–†–ï–¢–ù–´–ï —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.

–ö–õ–Æ–ß–ï–í–´–ï –ü–†–û–ë–õ–ï–ú–´:
{json.dumps(insights[:5], ensure_ascii=False, indent=2)}

–í–µ—Ä–Ω–∏ JSON:
{{
    "quick_wins": [
        {{
            "title": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ",
            "description": "–ß—Ç–æ –∏–º–µ–Ω–Ω–æ —Å–¥–µ–ª–∞—Ç—å",
            "implementation_steps": ["–®–∞–≥ 1", "–®–∞–≥ 2", "–®–∞–≥ 3"],
            "expected_impact": "–û–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç",
            "timeline": "–°—Ä–æ–∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏"
        }}
    ],
    "strategic_initiatives": [
        {{
            "title": "–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∞—è –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞",
            "description": "–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ",
            "expected_roi": "–û–∂–∏–¥–∞–µ–º–∞—è –æ—Ç–¥–∞—á–∞",
            "implementation_phases": ["–§–∞–∑–∞ 1", "–§–∞–∑–∞ 2"]
        }}
    ]
}}"""

        try:
            response = self.api_wrapper.generate_content(prompt, max_tokens=3000)
            return self.api_wrapper.extract_json(response)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return {"quick_wins": [], "strategic_initiatives": []}

    def _analyze_brief_questions(self, summaries: List[InterviewSummary], findings: ResearchFindings) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –±—Ä–∏—Ñ–∞"""
        if not self.brief_manager.has_brief:
            return {}

        questions = self.brief_manager.get_questions_for_analysis()
        if not questions:
            return {}

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ü–∏—Ç–∞—Ç—ã –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        all_quotes = []
        for summary in summaries:
            for quote in summary.quotes:
                if isinstance(quote, dict) and quote.get('text'):
                    all_quotes.append(f"–ò–Ω—Ç–µ—Ä–≤—å—é {summary.interview_id}: {quote['text']}")

        prompt = f"""–û—Ç–≤–µ—Ç—å –Ω–∞ –∫–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –∏–∑ –±—Ä–∏—Ñ–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω—Ç–µ—Ä–≤—å—é.

–í–û–ü–†–û–°–´ –ë–†–ò–§–ê:
{chr(10).join(f"{i+1}. {q}" for i, q in enumerate(questions))}

–î–ê–ù–ù–´–ï –ò–ó –ò–ù–¢–ï–†–í–¨–Æ:
{chr(10).join(all_quotes[:10])}

–í–µ—Ä–Ω–∏ JSON:
{{
    "answers": [
        {{
            "question": "–í–æ–ø—Ä–æ—Å –∏–∑ –±—Ä–∏—Ñ–∞",
            "answer": "–ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç —Å —Ü–∏—Ç–∞—Ç–∞–º–∏",
            "supporting_quotes": ["—Ü–∏—Ç–∞—Ç–∞ 1", "—Ü–∏—Ç–∞—Ç–∞ 2"],
            "confidence": "—É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (1-10)"
        }}
    ]
}}"""

        try:
            response = self.api_wrapper.generate_content(prompt, max_tokens=4000)
            return self.api_wrapper.extract_json(response)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤–æ–ø—Ä–æ—Å–æ–≤ –±—Ä–∏—Ñ–∞: {e}")
            return {"answers": []}

    def _assess_goal_achievement(self, findings: ResearchFindings, summaries: List[InterviewSummary]) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π"""
        if not self.brief_manager.has_brief:
            return {}

        goals = self.brief_manager.get_goals_for_analysis()
        if not goals:
            return {}

        return {
            "goals": [
                {
                    "goal": goal,
                    "achieved": "–ß–∞—Å—Ç–∏—á–Ω–æ",
                    "evidence": "–ù–∞–π–¥–µ–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ –∏–Ω—Ç–µ—Ä–≤—å—é",
                    "next_steps": "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑"
                }
                for goal in goals
            ]
        }

    def _format_insights_for_report(self, insights: List[Dict]) -> List[Dict]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –¥–ª—è –æ—Ç—á–µ—Ç–∞"""
        formatted_insights = []

        for insight in insights:
            formatted_insights.append({
                "title": insight.get("problem_title", ""),
                "description": insight.get("problem_description", ""),
                "severity": insight.get("severity", "medium"),
                "quotes": insight.get("quotes", []),
                "affected_percentage": insight.get("affected_percentage", "")
            })

        return formatted_insights
